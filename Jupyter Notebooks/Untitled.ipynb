{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from windrose import WindroseAxes\n",
    "\n",
    "\n",
    "# Preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset description:**\n",
    "\n",
    "1) Windfarm's data:\n",
    "    - 1 dataset from 2013 to 2016; data recorded every 10 min.\n",
    "    - 1 dataset from 2017 to January 2018; data recorded every 10 min.\n",
    "\n",
    "2) Meteorological station data:\n",
    "    - 1 dataset from 2013 to January 2018; data recorder every 3 hrs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Load Windfarm's data \n",
    "    - 1.1 Preliminary analysis\n",
    "    - 1.2 Concatenate windfarm dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File la-haute-borne-data-2013-2016.csv does not exist: 'la-haute-borne-data-2013-2016.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-595f71823026>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Dataset 1: windfarm's data between 2013 and 2016\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_13_16\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'la-haute-borne-data-2013-2016.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Dataset 2: windfarm's data between 2017 and 2018\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File la-haute-borne-data-2013-2016.csv does not exist: 'la-haute-borne-data-2013-2016.csv'"
     ]
    }
   ],
   "source": [
    "# Load windfarm's data:\n",
    "\n",
    "# Dataset 1: windfarm's data between 2013 and 2016\n",
    "df_13_16= pd.read_csv('la-haute-borne-data-2013-2016.csv', delimiter= ';')\n",
    "\n",
    "# Dataset 2: windfarm's data between 2017 and 2018\n",
    "df_17_18= pd.read_csv('la-haute-borne-data-2017-2020.csv', delimiter= ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 Preliminary analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head of df_13_16; 138 columns are displayed\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_13_16.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking shapes of windfarm's dataframes:\n",
    "\n",
    "print('Shape 2013-2016:', df_13_16.shape)\n",
    "print('Shape 2017-2018:', df_17_18.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of wind turbines and number records/turbine:\n",
    "df_13_16['Wind_turbine_name'].value_counts() # 4 turbines; 2013 - 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of wind turbines and number records/turbine:\n",
    "df_17_18['Wind_turbine_name'].value_counts() # 4 turbines 2017-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 Concat windfarm dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_13_16, df_17_18] \n",
    "df_turb = pd.concat(frames) # concat df_13_16 & df_17_18\n",
    "\n",
    "print('Shape of windfarm dataframe:', df_turb.shape)  # result shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete df_13_16 & df_17_18\n",
    "del df_13_16\n",
    "del df_17_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe by datetime and reset index:\n",
    "df_turb= df_turb.sort_values('Date_time')\n",
    "df_turb= df_turb.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Preprocessing \n",
    "    - 2.1 Deletion of features with high NaN %\n",
    "    - 2.2 Feature cleansing\n",
    "    - 2.3 Create 1 dataframe per turbine\n",
    "    - 2.4 Visualization of windfarm's data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Deletion of features with high NaN %**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate % of NaN\n",
    "nulls= df_turb.isnull().sum()/len(df_turb)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display columns and NaN %\n",
    "#pd.set_option('display.max_rows', None) # to display all rows\n",
    "nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append on a list features/columns with NaN ≥ 20%\n",
    "\n",
    "columns_high_NaN= []   # list\n",
    "\n",
    "for column, NaN in zip(df_turb.columns, nulls): # iterate through columns and % NaN\n",
    "    if NaN >= 20:  \n",
    "        columns_high_NaN.append(column) # appends column name if NaN >= 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with NaN ≥ 20% from list of column names \n",
    "df_turb= df_turb.drop(columns_high_NaN, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Feature cleansing**\n",
    "        \n",
    "    - The correlation matrix has been used to delete features that (1) show excessive           correlation between each others, (2) are too correlataed with P (power) and (3) their     correlation is too low with the P. \n",
    "       \n",
    "    - This process has been carried out with previous understanding of the significance each from feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap with correlation matrix from all windfarm's features\n",
    "# -- See 'heatmap in Jupyter Notebooks folder\n",
    "#sns.set(rc={'figure.figsize':(100,100)}) \n",
    "#sns.heatmap(df_turb.corr(), annot = True, cmap = \"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns non-correlated with the target (P)\n",
    "df_turb= df_turb.drop(['Rbt_std', 'Rs_std', 'Nu_std',\n",
    "                     'Nf_std',   'Nf_min',  'Nf_max', 'Nf_avg',\n",
    "                     'Va_std',   'Va_min',  'Va_max', 'Va_avg',\n",
    "                     'Wa_std',   'Wa_min',  'Ya_avg', 'Ya_min',\n",
    "                     'Ya_max',   'Gost_std','Git_std','Gb1t_std',\n",
    "                     'Gb2t_std', 'Db2t_std','Db2t_min','Db2t_max',\n",
    "                     'Db2t_avg', 'Db1t_std','Db1t_min','Db1t_max',\n",
    "                     'Db1t_avg', 'Cosphi_std','Cosphi_min','Cosphi_max',\n",
    "                     'Cosphi_avg','DCs_std', 'Rt_std', 'Rt_min', \n",
    "                     'Rt_max', 'Rt_avg', 'Ba_std'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that have excessive correlation (aprox 1 and -1) with another feature\n",
    "df_turb= df_turb.drop(['Wa_max', 'Nu_max', 'Nu_min', 'Nu_avg', 'Ya_std',\n",
    "                     'Ds_std', 'Rm_std', 'Rm_avg', 'Rm_max', 'Rm_min',\n",
    "                     'DCs_avg', 'DCs_max', 'DCs_min', 'Ds_avg', 'Ds_max',\n",
    "                     'Ds_min', 'Gb1t_max', 'Gb1t_min', 'Gb2t_min', 'Gb2t_max',\n",
    "                     'Dst_max', 'S_max', 'S_min', 'S_std', 'S_avg', 'Cm_max',\n",
    "                     'Cm_min', 'Cm_std', 'Cm_avg', 'Dst_std'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Create 1 dataframe per turbine**\n",
    "\n",
    "    - goal: concat with meteorological data each generator's dataframe\n",
    "    - fill missing values with interpolate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean 'Date_time' format\n",
    "def clean_date(string):\n",
    "    return \" \".join(string.split(\"+\")[0].split(\"T\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean \"Date_time\"\n",
    "df_turb[\"Date_time\"] = df_turb[\"Date_time\"].apply(clean_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of dataframes with 'Wind_turbine_name' as keys\n",
    "dict_frames = dict(tuple(df_turb.groupby('Wind_turbine_name')))\n",
    "\n",
    "# Save each dataframe within a new variable name\n",
    "df_turb11 = dict_frames['R80711']\n",
    "df_turb21 = dict_frames['R80721']\n",
    "df_turb36 = dict_frames['R80736']\n",
    "df_turb90 = dict_frames['R80790']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dict_frames # del dictionary to save some space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Visualization of windfarm's data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power vs Wind Speed. 4 turbines, each one with a different color\n",
    "sns.scatterplot(data=df_turb, x=\"Ws_avg\", y=\"P_avg\", hue=\"Wind_turbine_name\")\n",
    "sns.set(font_scale= 3)\n",
    "plt.title('Wind speed vs Power. 4 generators')\n",
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select values recorded when the machine is stopped but the anemometers still work\n",
    "sel_rows=df_turb[(df_turb['P_avg']<=0) &  (df_turb['Ws_avg']> 4.6)].index # where P <= 0 and Ws_avg > 4.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete selected rows from df_gen\n",
    "df_turb = df_turb.drop(sel_rows, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 'P_Avg' (power) vs rotor speed(RS), reactive power (Q_avg), stator temperature (DST), gearbox bearing temperature\n",
    "# (Gb1t), gearbox oil temperature (Gost), rotor bearing temperature (Rbt)\n",
    "for i in [ 'Rs_avg','Q_avg', 'Dst_avg', 'Gb1t_avg', 'Gost_avg','Rbt_avg']:\n",
    "    sns.set(font_scale=2)\n",
    "    sns.scatterplot(df_turb[i] ,df_turb.P_avg, color=\"r\")\n",
    "    plt.title('Power vs'+' '+i)\n",
    "    sns.set(rc={'figure.figsize':(10,10)})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Meteorological Data (independent station): processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv with meteorological data\n",
    "df_met = pd.read_csv('donnees-synop-essentielles-omm_TRUE.csv', delimiter= ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop static useless data, such as city name, department etc...\n",
    "df_met=df_met.drop(['Nom',          \n",
    "'ID OMM station','Altitude','Longitude','Latitude',\n",
    "'communes (name)','communes (code)','EPCI (name)',\n",
    "'EPCI (code)','department (name)','department (code)',\n",
    "'region (name)','region (code)','mois_de_l_annee','Coordonnees'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate % nulls per column\n",
    "nulls_meteo= df_met.isnull().sum()/len(df_met)*100\n",
    "for column, percentage in zip(df_met.columns, nulls_meteo):\n",
    "    print(column, percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append on a list features/columns with NaN ≥ 20%\n",
    "\n",
    "columns_high_NaN= []   # list\n",
    "\n",
    "for column, NaN in zip(df_met.columns, nulls_meteo): # iterate through columns and % NaN\n",
    "    if NaN >= 18:  \n",
    "        columns_high_NaN.append(column) # appends column name if NaN >= 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_met = df_met.drop(columns_high_NaN, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete irrelevant or repetitive information (already present in windfarm's data)\n",
    "df_met = df_met.drop(['Direction du vent moyen 10 mn', 'Vitesse du vent moyen 10 mn', \n",
    "                      'Vitesse du vent moyen 10 mn','Temps présent', \n",
    "                      'Periode de mesure de la rafale', 'Point de rosée', 'Type de tendance barométrique.1',\n",
    "                      'Temps présent.1', 'Température (°C)', 'Température' ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: clean date time column / erase '+'\n",
    "def clean_date(string):\n",
    "    return \" \".join(string.split(\"+\")[0].split(\"T\")) \n",
    "\n",
    "# Clean date\n",
    "df_met[\"Date\"] = df_met[\"Date\"].apply(clean_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort values by date \n",
    "df_met=df_met.sort_values(by='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After deleting several columns:\n",
    "df_met.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Feature Engineering \n",
    "\n",
    "    - 4.1 Time variables creation\n",
    "    - 4.2 Create the target: P_3h\n",
    "    - 4.3 Merge turbines with meteo\n",
    "    - 4.4 Interpolate missing meteorological records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1 Time variables creation**\n",
    "\n",
    "    - Calculate variables 6h, 3h, 1h before each time record --> rolling()\n",
    "    - Calculate TI (turbulence intensity) \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid default warning when applying .rolling()\n",
    "pd.options.mode.chained_assignment = None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function with rolling()\n",
    "def rolling_function(dataframe, new_column, variable, win_size):\n",
    "    dataframe[new_column]= dataframe[variable].rolling(win_size).mean()\n",
    "    return dataframe[new_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply rolling_function 3 times (1h, 3h, 6h) to: each df (dataframes) for 10 features (variables)\n",
    "dataframes = [df_turb11, df_turb21, df_turb36, df_turb90]\n",
    "variables = ['Git_avg', 'Ot_avg', 'Yt_avg', 'Rs_avg', 'Gb2t_avg',\n",
    "             'Gb1t_avg', 'Gost_avg', 'Wa_avg', 'Ba_avg', 'Rbt_avg']\n",
    "windows= [7, 19, 37]\n",
    "names_hours= ['1h', '3h', '6h']\n",
    "\n",
    "for dataframe in dataframes:\n",
    "    for variable in variables:\n",
    "        for window, name  in zip(windows, names_hours): # iterates through nº rows and names of new features\n",
    "\n",
    "            rolling_function(dataframe, variable + '_' + name, variable, int(window))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Turbulence intensity for each turbine --> std.dev Wind speed / avg Wind speed\n",
    "dataframes = [df_turb11, df_turb21, df_turb36, df_turb90]\n",
    "for d in dataframes: \n",
    "    d[\"TI\"] = d[\"Ws_std\"]/ d[\"Ws_avg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 Create the target: P_3h**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate target (P_3h) for each generator\n",
    "# Apply rolling on reverse mode to calculate P_3h (it takes 19 following rows)\n",
    "dataframes = [df_turb11, df_turb21, df_turb36, df_turb90]\n",
    "for d in dataframes:\n",
    "    d[\"P_3h\"] = d[\"P_avg\"].iloc[::-1].rolling(19).mean() \n",
    "    d.P_3h= d.P_3h.shift(periods=18) # Shift/push P_3h row 19 lines on each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_turb11= df_turb11.dropna(axis=0, subset=['P_3h'])\n",
    "df_turb21= df_turb21.dropna(axis=0, subset=['P_3h'])\n",
    "df_turb36= df_turb36.dropna(axis=0, subset=['P_3h'])\n",
    "df_turb90= df_turb90.dropna(axis=0, subset=['P_3h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'P_avg', 'P_min', 'P_max', 'P_std' to avoid overfitting; they were used to create our target(P_3h)\n",
    "dataframes = [df_turb11, df_turb21, df_turb36, df_turb90]\n",
    "for d in dataframes:\n",
    "    del d['P_avg']\n",
    "    del d['P_min']\n",
    "    del d['P_max']\n",
    "    del d['P_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_turb11.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_turb21.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3 Merge turbines with meteo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create common column 'datetime' in all generator frames and df_met \n",
    "dataframes = [df_turb11, df_turb21, df_turb36, df_turb90]\n",
    "for d in dataframes:\n",
    "    d.insert(2, 'datetime', d.Date_time) # 4 turbines\n",
    "df_met.insert(2, 'datetime', df_met.Date) # meteo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert 'datetime' column to_datetime type\n",
    "dataframes = [df_turb11, df_turb21, df_turb36, df_turb90, df_met]\n",
    "for d in dataframes:\n",
    "    d.datetime = pd.to_datetime(d.datetime)\n",
    "    d.set_index(\"datetime\",inplace=True) # Set 'datetime' column as index in all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge turbine's dataframes with df_met\n",
    "df_turb11= pd.merge(df_met, df_turb11, how=\"outer\", left_index=True, right_index=True)\n",
    "df_turb21= pd.merge(df_met, df_turb21, how=\"outer\", left_index=True, right_index=True)\n",
    "df_turb36= pd.merge(df_met, df_turb36, how=\"outer\", left_index=True, right_index=True)\n",
    "df_turb90= pd.merge(df_met, df_turb90, how=\"outer\", left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_turb11.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4 Interpolate missing meteorological records.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of frames and columns to interpolate\n",
    "d_frame= [df_turb11, df_turb21, df_turb36, df_turb90]\n",
    "c_columns= [\"Pression au niveau mer\", \"Variation de pression en 3 heures\", \"Humidité\", \n",
    "         \"Visibilité horizontale\", \"Pression station\", \"Rafales sur une période\", \n",
    "         \"Précipitations dans la dernière heure\", \"Précipitations dans les 3 dernières heures\"]\n",
    "\n",
    "# Interpolation of quantitative columns from df_met\n",
    "for d in d_frame:\n",
    "    for c in c_columns:\n",
    "        d[c]= d[c].interpolate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Work with all data: turbines + meteo\n",
    "\n",
    "    - 5.1 Concatenate all turbines with meteo\n",
    "    - 5.2 Processing all data\n",
    "    - 5.3 Create variables from months and years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1 Concatenate all turbines with meteo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate 3 dataframes without duplicates\n",
    "df_4turb= pd.concat([df_turb11, df_turb21, df_turb36, df_turb90]).drop_duplicates() #.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del [df_turb11, df_turb21, df_turb36, df_turb90, df_met] # save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4turb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2 Processing all data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_columns', None)\n",
    "df_4turb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop first 145 columns due to NaNs produced during rolling() function \n",
    "# 6h / turbine \n",
    "df_4turb= df_4turb.drop(df_4turb.index[range(0,144)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop due to high amount of NaN\n",
    "df_4turb= df_4turb.drop([\"Date\", \n",
    "                           \"Type de tendance barométrique\",\n",
    "                           \"Nébulosité  des nuages de l' étage inférieur\",\n",
    "                           \"index\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change names of columns from meteo\n",
    "col_dict= {'Pression au niveau mer': 'pr_sl', 'Variation de pression en 3 heures': 'pr_3h', \n",
    "         'Humidité': 'Hum', 'Visibilité horizontale': 'Vis_hor', 'Pression station': 'pr_st',\n",
    "         'Rafales sur une période': 'W_blast', 'Précipitations dans la dernière heure': 'Rain_1h',\n",
    "         'Précipitations dans les 3 dernières heures': 'Rain_3h',\n",
    "         'Hauteur de base 1': 'Hgt_base1'}\n",
    "\n",
    "df_4turb.columns = [col_dict.get(x, x) for x in df_4turb.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.3 Create variables from months and years**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4turb.Date_time = pd.to_datetime(df_4turb.Date_time)\n",
    "df_4turb['month']= df_4turb.Date_time.apply(lambda x : x.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'year' will only be used for statistical purposes; not for the model \n",
    "df_4turb['year']= df_4turb.Date_time.apply(lambda x : x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummies on 'month'\n",
    "dummy_variable = pd.get_dummies(df_4turb['month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat df with dummies\n",
    "df_4turb = pd.concat([df_4turb, dummy_variable], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_4turb['month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change names of columns from months\n",
    "col_dict2= {1.0: 'Jan', 2.0: 'Feb', \n",
    "         3.0: 'Mar', 4.0: 'Apr', 5.0: 'May',\n",
    "         6.0: 'Jun', 7.0: 'Jul',\n",
    "        8.0: 'Aug', 9.0: 'Sept',\n",
    "        10.0 : 'Oct' ,11.0: 'Nov', \n",
    "            12.0: 'Dec'}\n",
    "\n",
    "df_4turb.columns = [col_dict2.get(x, x) for x in df_4turb.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Correlation matrix between all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List comprehension to move target ('P_3h') to the right of the dataframe\n",
    "end_col = ['P_3h']\n",
    "df_4turb = df_4turb[[c for c in df_4turb if c not in end_col] \n",
    "        + [c for c in end_col if c in df_4turb]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Windrose graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windrose plot with filled representation of wind speeds (frequency) and wind directions\n",
    "# Yearly data \n",
    "list_years= [2013, 2014, 2015, 2016, 2017]\n",
    "for y in list_years:\n",
    "    year= y\n",
    "    figure = plt.figure(figsize=(12,12))\n",
    "    legend_position = 'upper left'\n",
    "    ax = figure.add_subplot(221,projection='windrose')\n",
    "    ax.contourf(df_4turb.loc[df_4turb['year']== year,'Wa_avg'], df_4turb.loc[df_4turb['year']==year,'Ws_avg'], \n",
    "                 bins = np.arange(2, 12, 2),cmap=cm.hot)\n",
    "    ax.contour(df_4turb.loc[df_4turb['year']==year,'Wa_avg'], df_4turb.loc[df_4turb['year']==year,'Ws_avg'],\n",
    "                colors='black',bins = np.arange(2, 12, 2))\n",
    "    ax.set_legend(loc= legend_position, fontsize= 18)\n",
    "    plt.title('Year:' +' '+ str(y), y=1.1, fontsize= 15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter all months separatedly from the 5 years of records\n",
    "jan= df_4turb[df_4turb['Jan']==1]\n",
    "feb= df_4turb[df_4turb['Feb']==1]\n",
    "mar= df_4turb[df_4turb['Mar']==1]\n",
    "apr= df_4turb[df_4turb['Apr']==1]\n",
    "may= df_4turb[df_4turb['May']==1]\n",
    "jun= df_4turb[df_4turb['Jun']==1]\n",
    "jul= df_4turb[df_4turb['Jul']==1]\n",
    "aug= df_4turb[df_4turb['Aug']==1]\n",
    "sept= df_4turb[df_4turb['Sept']==1]\n",
    "octb = df_4turb[df_4turb['Oct']==1]\n",
    "nov= df_4turb[df_4turb['Nov']==1]\n",
    "dec= df_4turb[df_4turb['Dec']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked histogram with wind directions and speeds\n",
    "# Data separated by months from all the years that we have records\n",
    "dfs_months = [jan, feb, mar, apr, may, jun, jul, aug, sept, octb, nov, dec]\n",
    "labels =['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "for i,j in zip(dfs_months,labels):\n",
    "\n",
    "    ax = WindroseAxes.from_ax()\n",
    "    ax.bar(i['Wa_avg'], i['Ws_avg'], normed=True, opening=0.8, edgecolor='white',cmap=cm.hot)\n",
    "    ax.legend(bbox_to_anchor=(1.02, 0))\n",
    "    ax.set_title((j), y=1, fontsize= 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save some memory space\n",
    "del [jan, feb, mar, apr, may, jun, jul, aug, sept, octb, nov, dec, df_4turb['year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Last processing before modelling \n",
    "\n",
    "    - 8.1 Check for infinites and substitute\n",
    "    - 8.2 Create dataframe from a single turbine\n",
    "    - 8.3 Remove outliers with IQR\n",
    "    - 8.4 Extract the target and save it as CSV\n",
    "    - 8.5 Standarize\n",
    "    - 8.6 NaN imputation: KNNimputer\n",
    "    - 8.7 Export data for modelling as CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.1 Check for infinites and substitute**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitute possible infinites (due to division by zero) for nan\n",
    "df_4turb= df_4turb.replace(np.inf, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2 Create dataframe from a single turbine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exctract a dataframe with a single generator\n",
    "Turb11= df_4turb[df_4turb['Wind_turbine_name']=='R80711']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Turb11['Wind_turbine_name']\n",
    "del Turb11['Date_time']\n",
    "del df_4turb['Wind_turbine_name']\n",
    "del df_4turb['Date_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.3 Remove outliers with IQR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data seems to be very homogeneous; if we settle Q1= 0.18 & Q3= 0.95 we loose more than 95% od data\n",
    "Q1= df_4turb.quantile(0.05)\n",
    "Q3= df_4turb.quantile(0.95)\n",
    "IQR= Q3-Q1 # IQR is the first quartile subtracted from the third quartile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using IQR \n",
    "df_4turb = df_4turb[~((df_4turb < (Q1 - 1.5 * IQR)) |(df_4turb > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "df_4turb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR for a single turbine\n",
    "Q1= Turb11.quantile(0.05)\n",
    "Q3= Turb11.quantile(0.95)\n",
    "IQR= Q3-Q1 \n",
    "Turb11 = Turb11[~((Turb11 < (Q1 - 1.5 * IQR)) |(Turb11 > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "Turb11.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.4 Extract the target and save it as CSV**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4 Turbines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save copy of our target \n",
    "P_3h= df_4turb['P_3h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_3h= pd.DataFrame(P_3h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_3h.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_3h.to_csv('P_3h.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_4turb['P_3h']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1 Turbine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_3h_Turb11= Turb11['P_3h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y to dataframe\n",
    "P_3h_Turb11= pd.DataFrame(P_3h_Turb11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_3h_Turb11.to_csv('P_3h_Turb11.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Turb11['P_3h']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.5 Standarize**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4 Turbines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(df_4turb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.DataFrame(scaled, columns=df_4turb.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1 Turbine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize\n",
    "scaler_turb11 = StandardScaler()\n",
    "scaled_turb11 = scaler_turb11.fit_transform(Turb11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Turb11= pd.DataFrame(scaled_turb11, columns=Turb11.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.6 NaN imputation: KNNimputer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4 Turbines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNNImputer for Nans\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "imputed = imputer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.DataFrame(imputed, columns=df_4turb.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1 Turbine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNNImputer for Nans\n",
    "imputer_turb11 = KNNImputer(n_neighbors=3)\n",
    "imputed_turb11 = imputer_turb11.fit_transform(df_Turb11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Turb11= pd.DataFrame(imputed_turb11, columns=Turb11.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.7 Export data for modelling as CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV with 4 turbine's data\n",
    "df.to_csv('4Gen_Model.csv', index=True)\n",
    "# CSV with 1 turbine's data\n",
    "df_Turb11.to_csv('Turb11_Model.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del [df, df_Turb11]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
